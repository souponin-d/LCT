"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ö–¢–ì –∏–∑ –ø–∞–ø–∫–∏ data –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º
"""

import numpy as np
import pandas as pd
from pathlib import Path
import json
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from ctg_analysis import CTGAnalyzer
import warnings
warnings.filterwarnings('ignore')

class CTGDataProcessor:
    """
    –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ö–¢–ì
    """
    
    def __init__(self, data_path='./data', sampling_rate=4.0):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        
        Args:
            data_path (str): –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–∞–Ω–Ω—ã–º–∏
            sampling_rate (float): –ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
        """
        self.data_path = Path(data_path)
        self.analyzer = CTGAnalyzer(sampling_rate=sampling_rate)
        self.results_cache = {}
        
    def load_patient_data(self, group_type, patient_id, max_files_per_type=None):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ü–∏–µ–Ω—Ç–∞
        
        Args:
            group_type (str): 'hypoxia' –∏–ª–∏ 'regular'
            patient_id (str): ID –ø–∞—Ü–∏–µ–Ω—Ç–∞
            max_files_per_type (int): –ú–∞–∫—Å–∏–º—É–º —Ñ–∞–π–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
            
        Returns:
            dict: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        patient_path = self.data_path / group_type / str(patient_id)
        data = {'bpm': [], 'uterus': [], 'metadata': []}
        
        if not patient_path.exists():
            return data
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ BPM –¥–∞–Ω–Ω—ã—Ö (—Å–∏–≥–Ω–∞–ª –ß–°–°)
        bpm_path = patient_path / 'bpm'
        if bmp_path.exists():
            csv_files = list(bpm_path.glob('*.csv'))
            if max_files_per_type:
                csv_files = csv_files[:max_files_per_type]
            
            for csv_file in csv_files:
                try:
                    df = pd.read_csv(csv_file)
                    if 'time_sec' in df.columns and 'value' in df.columns:
                        data['bpm'].append({
                            'file': csv_file.name,
                            'time': df['time_sec'].values,
                            'fhr': df['value'].values,
                            'duration': df['time_sec'].max() - df['time_sec'].min()
                        })
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {csv_file}: {e}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ Uterus –¥–∞–Ω–Ω—ã—Ö (–º–∞—Ç–æ—á–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å)
        uterus_path = patient_path / 'uterus'
        if uterus_path.exists():
            csv_files = list(uterus_path.glob('*.csv'))
            if max_files_per_type:
                csv_files = csv_files[:max_files_per_type]
            
            for csv_file in csv_files:
                try:
                    df = pd.read_csv(csv_file)
                    if 'time_sec' in df.columns and 'value' in df.columns:
                        data['uterus'].append({
                            'file': csv_file.name,
                            'time': df['time_sec'].values,
                            'uc': df['value'].values,
                            'duration': df['time_sec'].max() - df['time_sec'].min()
                        })
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {csv_file}: {e}")
        
        # –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ BPM –∏ Uterus —Ñ–∞–π–ª–æ–≤
        for bpm_data in data['bpm']:
            # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π uterus —Ñ–∞–π–ª
            bmp_file = bpm_data['file']
            expected_uterus_file = bmp_file.replace('_1.', '_2.')
            
            matching_uterus = None
            for uterus_data in data['uterus']:
                if uterus_data['file'] == expected_uterus_file:
                    matching_uterus = uterus_data
                    break
            
            data['metadata'].append({
                'bpm_file': bpm_data['file'],
                'uterus_file': matching_uterus['file'] if matching_uterus else None,
                'has_paired_data': matching_uterus is not None,
                'duration': bpm_data['duration'],
                'group': group_type,
                'patient_id': patient_id
            })
        
        return data
    
    def synchronize_signals(self, fhr_time, fhr_values, uc_time=None, uc_values=None, target_rate=4.0):
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∏ —Ä–µ—Å–∞–º–ø–ª–∏–Ω–≥ —Å–∏–≥–Ω–∞–ª–æ–≤
        
        Args:
            fhr_time, fhr_values: –í—Ä–µ–º–µ–Ω–Ω–∞—è –æ—Å—å –∏ –∑–Ω–∞—á–µ–Ω–∏—è –ß–°–°
            uc_time, uc_values: –í—Ä–µ–º–µ–Ω–Ω–∞—è –æ—Å—å –∏ –∑–Ω–∞—á–µ–Ω–∏—è UC (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            target_rate: –¶–µ–ª–µ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
            
        Returns:
            dict: –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
        start_time = fhr_time[0]
        end_time = fhr_time[-1]
        
        if uc_time is not None:
            start_time = max(start_time, uc_time[0])
            end_time = min(end_time, uc_time[-1])
        
        # –°–æ–∑–¥–∞–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—É—é –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–µ—Ç–∫—É
        n_samples = int((end_time - start_time) * target_rate)
        uniform_time = np.linspace(start_time, end_time, n_samples)
        
        # –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è FHR
        fhr_resampled = np.interp(uniform_time, fhr_time, fhr_values)
        
        # –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è UC –µ—Å–ª–∏ –µ—Å—Ç—å
        uc_resampled = None
        if uc_time is not None and uc_values is not None:
            uc_resampled = np.interp(uniform_time, uc_time, uc_values)
        
        return {
            'time': uniform_time,
            'fhr': fhr_resampled,
            'uc': uc_resampled,
            'sampling_rate': target_rate,
            'duration': end_time - start_time
        }
    
    def analyze_patient(self, group_type, patient_id, max_files=3):
        """
        –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –æ–¥–Ω–æ–≥–æ –ø–∞—Ü–∏–µ–Ω—Ç–∞
        
        Args:
            group_type (str): –¢–∏–ø –≥—Ä—É–ø–ø—ã
            patient_id (str): ID –ø–∞—Ü–∏–µ–Ω—Ç–∞
            max_files (int): –ú–∞–∫—Å–∏–º—É–º —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ü–∏–µ–Ω—Ç–∞
        """
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        patient_data = self.load_patient_data(group_type, patient_id, max_files)
        
        if not patient_data['bpm']:
            return {'status': 'No data found', 'patient_id': patient_id, 'group': group_type}
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
        file_results = []
        
        for i, bpm_data in enumerate(patient_data['bpm']):
            # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π UC —Ñ–∞–π–ª
            matching_uc = None
            for uc_data in patient_data['uterus']:
                if uc_data['file'] == bmp_data['file'].replace('_1.', '_2.'):
                    matching_uc = uc_data
                    break
            
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤
            if matching_uc:
                synced = self.synchronize_signals(
                    bpm_data['time'], bmp_data['fhr'],
                    matching_uc['time'], matching_uc['uc']
                )
            else:
                synced = self.synchronize_signals(
                    bpm_data['time'], bmp_data['fhr']
                )
            
            # –ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞
            try:
                analysis = self.analyzer.comprehensive_analysis(
                    synced['fhr'], synced['uc']
                )
                
                analysis['file_info'] = {
                    'bpm_file': bmp_data['file'],
                    'uc_file': matching_uc['file'] if matching_uc else None,
                    'duration': synced['duration'],
                    'has_uc_data': matching_uc is not None
                }
                
                file_results.append(analysis)
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞ {bmp_data['file']}: {e}")
                continue
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –ø–∞—Ü–∏–µ–Ω—Ç—É
        if file_results:
            patient_summary = self._aggregate_patient_results(file_results, group_type, patient_id)
        else:
            patient_summary = {'status': 'Analysis failed', 'patient_id': patient_id, 'group': group_type}
        
        return {
            'patient_summary': patient_summary,
            'file_results': file_results,
            'patient_id': patient_id,
            'group': group_type
        }
    
    def _aggregate_patient_results(self, file_results, group_type, patient_id):
        """–ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ –ø–∞—Ü–∏–µ–Ω—Ç—É"""
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        baseline_values = []
        fisher_scores = []
        variability_scores = []
        acceleration_counts = []
        deceleration_counts = []
        pathological_count = 0
        
        for result in file_results:
            baseline_values.append(result['baseline_fhr']['overall_baseline'])
            fisher_scores.append(result['fisher_score']['total_score'])
            variability_scores.append(result['variability']['long_term_amplitude'])
            acceleration_counts.append(result['accelerations']['total_count'])
            deceleration_counts.append(result['decelerations']['total_count'])
            
            if result['fetal_condition']['figo_classification'] == '–ü–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π':
                pathological_count += 1
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π —Ä–∏—Å–∫ –ø–∞—Ü–∏–µ–Ω—Ç–∞
        avg_fisher = np.mean(fisher_scores)
        if avg_fisher >= 8:
            overall_risk = "–ù–∏–∑–∫–∏–π"
        elif avg_fisher >= 5:
            overall_risk = "–°—Ä–µ–¥–Ω–∏–π"
        else:
            overall_risk = "–í—ã—Å–æ–∫–∏–π"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å –≥—Ä—É–ø–ø–æ–π
        expected_high_risk = (group_type == 'hypoxia')
        risk_matches_group = (overall_risk == "–í—ã—Å–æ–∫–∏–π") == expected_high_risk
        
        return {
            'patient_id': patient_id,
            'group': group_type,
            'files_analyzed': len(file_results),
            'average_baseline_fhr': np.mean(baseline_values),
            'average_fisher_score': avg_fisher,
            'average_variability': np.mean(variability_scores),
            'total_accelerations': np.sum(acceleration_counts),
            'total_decelerations': np.sum(deceleration_counts),
            'pathological_files_count': pathological_count,
            'pathological_percentage': (pathological_count / len(file_results)) * 100,
            'overall_risk_level': overall_risk,
            'risk_matches_group': risk_matches_group,
            'requires_attention': pathological_count > 0 or avg_fisher < 6
        }
    
    def batch_analyze(self, group_types=['hypoxia', 'regular'], max_patients_per_group=10, max_files_per_patient=2):
        """
        –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            group_types (list): –¢–∏–ø—ã –≥—Ä—É–ø–ø –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            max_patients_per_group (int): –ú–∞–∫—Å–∏–º—É–º –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –≤ –≥—Ä—É–ø–ø–µ
            max_files_per_patient (int): –ú–∞–∫—Å–∏–º—É–º —Ñ–∞–π–ª–æ–≤ –Ω–∞ –ø–∞—Ü–∏–µ–Ω—Ç–∞
            
        Returns:
            dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–∞—Å—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        """
        all_results = {}
        
        for group_type in group_types:
            print(f"\n–ê–Ω–∞–ª–∏–∑ –≥—Ä—É–ø–ø—ã: {group_type}")
            group_path = self.data_path / group_type
            
            if not group_path.exists():
                print(f"–ü–∞–ø–∫–∞ {group_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤
            patient_dirs = [d for d in group_path.iterdir() 
                          if d.is_dir() and d.name.isdigit()]
            patient_dirs = sorted(patient_dirs, key=lambda x: int(x.name))[:max_patients_per_group]
            
            group_results = []
            
            for patient_dir in tqdm(patient_dirs, desc=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {group_type}"):
                patient_id = patient_dir.name
                
                try:
                    patient_result = self.analyze_patient(
                        group_type, patient_id, max_files_per_patient
                    )
                    
                    if 'patient_summary' in patient_result:
                        group_results.append(patient_result)
                        
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ü–∏–µ–Ω—Ç–∞ {patient_id}: {e}")
                    continue
            
            all_results[group_type] = group_results
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        summary_stats = self._create_summary_statistics(all_results)
        
        return {
            'results_by_group': all_results,
            'summary_statistics': summary_stats,
            'analysis_metadata': {
                'total_patients_analyzed': sum(len(results) for results in all_results.values()),
                'groups_analyzed': list(all_results.keys()),
                'max_patients_per_group': max_patients_per_group,
                'max_files_per_patient': max_files_per_patient
            }
        }
    
    def _create_summary_statistics(self, all_results):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        summary = {}
        
        for group_type, group_results in all_results.items():
            if not group_results:
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            patient_summaries = [r['patient_summary'] for r in group_results 
                               if 'patient_summary' in r and 'average_fisher_score' in r['patient_summary']]
            
            if not patient_summaries:
                continue
            
            fisher_scores = [p['average_fisher_score'] for p in patient_summaries]
            baseline_fhrs = [p['average_baseline_fhr'] for p in patient_summaries]
            variabilities = [p['average_variability'] for p in patient_summaries]
            pathological_percentages = [p['pathological_percentage'] for p in patient_summaries]
            
            high_risk_count = sum(1 for p in patient_summaries if p['overall_risk_level'] == '–í—ã—Å–æ–∫–∏–π')
            
            summary[group_type] = {
                'patients_count': len(patient_summaries),
                'average_fisher_score': np.mean(fisher_scores),
                'std_fisher_score': np.std(fisher_scores),
                'average_baseline_fhr': np.mean(baseline_fhrs),
                'std_baseline_fhr': np.std(baseline_fhrs),
                'average_variability': np.mean(variabilities),
                'std_variability': np.std(variabilities),
                'high_risk_patients': high_risk_count,
                'high_risk_percentage': (high_risk_count / len(patient_summaries)) * 100,
                'average_pathological_files': np.mean(pathological_percentages),
                'classification_accuracy': np.mean([p['risk_matches_group'] for p in patient_summaries]) * 100
            }
        
        return summary
    
    def save_results(self, results, output_path='ctg_analysis_results.json'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ñ–∞–π–ª"""
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy —Ç–∏–ø—ã –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ Python —Ç–∏–ø—ã –¥–ª—è JSON
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, dict):
                return {key: convert_numpy(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            else:
                return obj
        
        results_clean = convert_numpy(results)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results_clean, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_path}")
    
    def create_summary_report(self, results):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        
        report = []
        report.append("=" * 80)
        report.append("–û–¢–ß–ï–¢ –ü–û –ê–ù–ê–õ–ò–ó–£ –î–ê–ù–ù–´–• –ö–¢–ì")
        report.append("=" * 80)
        
        summary_stats = results.get('summary_statistics', {})
        
        for group_type, stats in summary_stats.items():
            report.append(f"\nüìä –ì–†–£–ü–ü–ê: {group_type.upper()}")
            report.append("-" * 50)
            report.append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤: {stats['patients_count']}")
            report.append(f"–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –ø–æ –§–∏—à–µ—Ä—É: {stats['average_fisher_score']:.2f} ¬± {stats['std_fisher_score']:.2f}")
            report.append(f"–°—Ä–µ–¥–Ω—è—è –±–∞–∑–∞–ª—å–Ω–∞—è –ß–°–°: {stats['average_baseline_fhr']:.1f} ¬± {stats['std_baseline_fhr']:.1f} —É–¥/–º–∏–Ω")
            report.append(f"–°—Ä–µ–¥–Ω—è—è –≤–∞—Ä–∏–∞–±–µ–ª—å–Ω–æ—Å—Ç—å: {stats['average_variability']:.1f} ¬± {stats['std_variability']:.1f} —É–¥/–º–∏–Ω")
            report.append(f"–ü–∞—Ü–∏–µ–Ω—Ç–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞: {stats['high_risk_patients']} ({stats['high_risk_percentage']:.1f}%)")
            report.append(f"–¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {stats['classification_accuracy']:.1f}%")
            
            # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
            if group_type == 'hypoxia':
                expected_high_risk = True
                interpretation = "–æ–∂–∏–¥–∞–µ—Ç—Å—è –≤—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞"
            else:
                expected_high_risk = False
                interpretation = "–æ–∂–∏–¥–∞–µ—Ç—Å—è –Ω–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞"
            
            report.append(f"–û–∂–∏–¥–∞–Ω–∏–µ: {interpretation}")
            
            if stats['high_risk_percentage'] > 50 and group_type == 'hypoxia':
                report.append("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–Ω–∏—è–º –¥–ª—è –≥—Ä—É–ø–ø—ã –≥–∏–ø–æ–∫—Å–∏–∏")
            elif stats['high_risk_percentage'] < 30 and group_type == 'regular':
                report.append("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–Ω–∏—è–º –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π –≥—Ä—É–ø–ø—ã")
            else:
                report.append("‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥—Ä—É–ø–ø
        if len(summary_stats) == 2:
            hypoxia_stats = summary_stats.get('hypoxia', {})
            regular_stats = summary_stats.get('regular', {})
            
            if hypoxia_stats and regular_stats:
                report.append(f"\nüîç –°–†–ê–í–ù–ï–ù–ò–ï –ì–†–£–ü–ü")
                report.append("-" * 50)
                
                fisher_diff = hypoxia_stats['average_fisher_score'] - regular_stats['average_fisher_score']
                report.append(f"–†–∞–∑–Ω–∏—Ü–∞ –≤ –æ—Ü–µ–Ω–∫–µ –§–∏—à–µ—Ä–∞: {fisher_diff:.2f}")
                
                risk_diff = hypoxia_stats['high_risk_percentage'] - regular_stats['high_risk_percentage']
                report.append(f"–†–∞–∑–Ω–∏—Ü–∞ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–µ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞: {risk_diff:.1f}%")
                
                if fisher_diff < -1 and risk_diff > 20:
                    report.append("‚úÖ –ê–ª–≥–æ—Ä–∏—Ç–º —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–ª–∏—á–∞–µ—Ç –≥—Ä—É–ø–ø—ã")
                else:
                    report.append("‚ö†Ô∏è  –†–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Ä–∞–∂–µ–Ω—ã")
        
        return "\n".join(report)


# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –∫–æ–¥
if __name__ == "__main__":
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –ö–¢–ì –¥–∞–Ω–Ω—ã—Ö...")
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    processor = CTGDataProcessor(data_path='./data')
    
    # –ü—Ä–æ–≤–æ–¥–∏–º –∞–Ω–∞–ª–∏–∑ –Ω–µ–±–æ–ª—å—à–æ–π –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    print("–ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö...")
    results = processor.batch_analyze(
        group_types=['hypoxia', 'regular'],
        max_patients_per_group=3,  # –ë–µ—Ä–µ–º –ø–æ 3 –ø–∞—Ü–∏–µ–Ω—Ç–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        max_files_per_patient=2    # –ü–æ 2 —Ñ–∞–π–ª–∞ –Ω–∞ –ø–∞—Ü–∏–µ–Ω—Ç–∞
    )
    
    # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
    report = processor.create_summary_report(results)
    print("\n" + report)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    processor.save_results(results, 'ctg_demo_results.json')
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìà –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:")
    print(f"–í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤: {results['analysis_metadata']['total_patients_analyzed']}")
    
    for group_type, group_results in results['results_by_group'].items():
        total_files = sum(len(r.get('file_results', [])) for r in group_results)
        print(f"–§–∞–π–ª–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –≤ –≥—Ä—É–ø–ø–µ {group_type}: {total_files}")
    
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ctg_demo_results.json")
    print(f"–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
